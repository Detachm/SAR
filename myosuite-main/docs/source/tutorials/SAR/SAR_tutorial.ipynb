{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3084a00",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 42px;\">SAR tutorial</h1>\n",
    "\n",
    "In this notebook, we introduce synergistic action representations (SAR) and demonstrate how to use these representations to facilitate the training of high-dimensional continuous control policies within MyoSuite.\n",
    "\n",
    "The primary purpose of this notebook is to equip the user to train with SAR for their own task specification(s) both within and beyond the musculoskeletal control paradigm.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h1 style=\"font-size: 28px;\">Contents of this notebook</h1>\n",
    "\n",
    "<ul style=\"line-height: 2; font-size: 16px;\">\n",
    "  <li><b>Imports and utilities</b>: set up tutorial with relevant imports and starter functions.\n",
    "  </li>\n",
    "  <li><b>Walkthrough of SAR core functions</b>: steps through and defines each of the four basic functions of the SAR method.\n",
    "  </li>\n",
    "  <li><b>Example 1: SAR x physiological locomotion</b>: end-to-end training example using the SAR method to learn forward locomotion on a diverse set of terrains.\n",
    "  </li>\n",
    "  <li><b>Example 2: SAR x physiological dexterity</b>: end-to-end training example using the SAR method to learn multiobject manipulation of parametric geometries.\n",
    "  </li>\n",
    "</ul>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5eddd",
   "metadata": {},
   "source": [
    "# Imports and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fe8e7",
   "metadata": {},
   "source": [
    "First, we import our required and helper functions from `SAR_tutorial_utils.py`. If you encounter import errors, it is recommended to `pip install` any imports that are missing in your particular environment by uncommenting out the appropriate lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8526f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3==1.7.0\n",
    "# !pip install joblib\n",
    "# !pip install scikit-learn\n",
    "# !pip install tqdm\n",
    "# !pip install matplotlib\n",
    "# !pip install gymnasium\n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'glfw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60114185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyoSuite:> Registering Myo Envs\n",
      "使用 MPS 设备加速训练\n"
     ]
    }
   ],
   "source": [
    "# imports for SAR\n",
    "from SAR_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc299ac0",
   "metadata": {},
   "source": [
    "# SAR core functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8433315",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "         Next, implement for SAR step by step. The framework follows the basic structure displayed below. Accordingly, we will implement functions that do each of the following:\n",
    "        <br>\n",
    "        <br>\n",
    "        <ol>\n",
    "            <li>Train a play phase policy.</li>\n",
    "            <li>Roll out play phase policy to capture muscle activations over time</li>\n",
    "            <li>Compute SAR from this activation dataset</li>\n",
    "            <li>Use computed SAR to train on target task</li>\n",
    "        </ol>\n",
    "        <br>\n",
    "        As we implement each of the above, we will also go into more detail about best practices for that step.\n",
    "        <br>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8cadd",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/framework.png\" alt=\"SAR framework\" style=\"width:1200px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c125c47",
   "metadata": {},
   "source": [
    "### Step 1: Train a play phase policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24053a43",
   "metadata": {},
   "source": [
    "When selecting a play phase environment, it is advised to select a task that is both (a) simple enough that vanilla RL will be able to make reasonable progress on the task, and (b) sufficiently similar to the target task such that a synergistic representation learned from this simpler task will actually be informative for the target behavior. For example, in our manipulation pipeline, the play phase is a reorientation task composed of a small number of geometries, while the target reorientation task is composed of a much larger set of geometries. It is helpful to think of the play phase as a kind of curriculum learning that 'eases' the agent into learning useful representations for the target task.\n",
    "\n",
    "`train()` enables the simple training of a policy using the stable-baselines3 implementation of SAC on a desired environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a94b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, policy_name, timesteps, seed):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=1000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30fc82",
   "metadata": {},
   "source": [
    "### Step 2: Roll out play phase policy to capture muscle activations over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350c00b",
   "metadata": {},
   "source": [
    "`get_activations()` extracts muscle activation data at each timestep of the trained play phase policy's rollouts. In this implementation, `get_activations()` captures muscle activations from a given rollout if the rewards from that rollout fall above a certain threshold, computed as a percentile from a set of preview rollouts. \n",
    "\n",
    "Here, we set the number of sample episodes to 2000 and the reward percentile cutoff at 80%, though these should be considered hyperparameters that may require finetuning for a specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be60e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(name, env_name, seed, episodes=2000, percentile=80):\n",
    "    \"\"\"\n",
    "    Returns muscle activation data from N runs of a trained policy.\n",
    "\n",
    "    name: str; policy name (see train())\n",
    "    env_name: str; name of the gym environment\n",
    "    seed: str; seed of the trained policy\n",
    "    episodes: int; optional; how many rollouts?\n",
    "    percentile: int; optional; percentile to set the reward threshold for considering an episode as successful\n",
    "    \"\"\"\n",
    "    with gym.make(env_name) as env:\n",
    "        env.reset()\n",
    "\n",
    "        model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "        vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "\n",
    "        # Calculate the reward threshold from 100 preview episodes\n",
    "        preview_rewards = []\n",
    "        for _ in range(100):\n",
    "            env.reset()\n",
    "            rewards = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, *_, info = env.step(a)\n",
    "                rewards += r\n",
    "            preview_rewards.append(rewards)\n",
    "        reward_threshold = np.percentile(preview_rewards, percentile)\n",
    "\n",
    "        # Run the main episode loop\n",
    "        solved_acts = []\n",
    "        for _ in tqdm(range(episodes)):\n",
    "            env.reset()\n",
    "            rewards, acts = 0, []\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                o = env.get_obs()\n",
    "                o = vec.normalize_obs(o)\n",
    "                a, __ = model.predict(o, deterministic=False)\n",
    "                next_o, r, done, *_, info = env.step(a)\n",
    "                acts.append(env.sim.data.act.copy())\n",
    "                rewards += r\n",
    "\n",
    "            if rewards > reward_threshold:\n",
    "                solved_acts.extend(acts)\n",
    "\n",
    "    return np.array(solved_acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc01ec",
   "metadata": {},
   "source": [
    "### Step 3: Compute SAR from the activation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af3cdc",
   "metadata": {},
   "source": [
    "We use linear representations (i.e., PCA, ICA, normalization) to approximate SAR. These representations are based directly on the motor neuroscience literature of representing muscle synergies (c.f., [Tresch et al, 2006](https://journals.physiology.org/doi/epdf/10.1152/jn.00222.2005)). Such representations also have the advantages of being highly interpretable and efficient for use as control signals. We also found in practice that nonlinear representations (such as VAE decoder networks) did not lead to strong performance are were far less efficient for training.\n",
    "\n",
    "Before computing SAR, we first seek to understand how informative each individual muscle synergy is for explaining the initial muscle activation dataset. `find_synergies` returns a dictionary (and optionally, a plot) that shows variance accounted for (VAF) by N synergies from the original muscle activation data. In general, we find that using a number of synergies where VAF > 80% leads to good performance, though this value should be treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083dc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synergies(acts, plot=True):\n",
    "    \"\"\"\n",
    "    Computed % variance explained in the original muscle activation data with N synergies.\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    plot: bool; whether to plot the result\n",
    "    \"\"\"\n",
    "    syn_dict = {}\n",
    "    for i in range(acts.shape[1]):\n",
    "        pca = PCA(n_components=i+1)\n",
    "        _ = pca.fit_transform(acts)\n",
    "        syn_dict[i+1] =  round(sum(pca.explained_variance_ratio_), 4)\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(list(syn_dict.keys()), list(syn_dict.values()))\n",
    "        plt.title('VAF by N synergies')\n",
    "        plt.xlabel('# synergies')\n",
    "        plt.ylabel('VAF')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    return syn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13361813",
   "metadata": {},
   "source": [
    "Once the number of synergies to use is determined (above), `compute_SAR()` will take the activation data as input and yield the synergistic action representation (SAR) in the form of ICA, PCA, and normalizer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8da034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SAR(acts, n_syn, save=False):\n",
    "    \"\"\"\n",
    "    Takes activation data and desired n_comp as input and returns/optionally saves the ICA, PCA, and Scaler objects\n",
    "    \n",
    "    acts: np.array; rollout data containing the muscle activations\n",
    "    n_comp: int; number of synergies to use\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_syn)\n",
    "    pca_act = pca.fit_transform(acts)\n",
    "    \n",
    "    ica = FastICA()\n",
    "    pcaica_act = ica.fit_transform(pca_act)\n",
    "    \n",
    "    normalizer = MinMaxScaler((-1, 1))    \n",
    "    normalizer.fit(pcaica_act)\n",
    "    \n",
    "    if save:\n",
    "        joblib.dump(ica, 'ica_demo.pkl') \n",
    "        joblib.dump(pca, 'pca_demo.pkl')\n",
    "        joblib.dump(normalizer, 'scaler_demo.pkl')\n",
    "    \n",
    "    return ica, pca, normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60f8d5",
   "metadata": {},
   "source": [
    "### Step 4: Use computed SAR to train on target task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c987cc",
   "metadata": {},
   "source": [
    "In order to train with SAR, we initialize a `SynNoSynWrapper`, which implements the policy architecture presented below. \n",
    "\n",
    "The policy takes as input $o_t$ and outputs an $O+N$-dimensional action vector, $a_{t}^{O+N}$, where $O$ is the dimensionality of the original action manifold and $N$ is the dimensionality of the synergistic manifold. The first $N$ synergistic actions are passed through $SAR(a_{t}^{N})=a_{t}^{SAR}$ to recover the synergistic muscle activations. The subsequent $O$ task-specific activations are mixed with the task-general activations using a linear blend $\\varphi$ to recover the final action $a_t^*$ that steps the environment forward. Note that for the locomotion experiments presented next, it was actually found that purely relying on the synergistic, task-general pathway ($\\varphi=1$) was sufficient for learning robust locomotion (see end-to-end example later in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d499c17",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;\">\n",
    "    <img src=\"./SAR_images/pol_arch.png\" alt=\"Policy Architecture\" style=\"width:850px; display:block; margin-left:auto; margin-right:auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997767da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynNoSynWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the combination of a task-general synergy space and a\n",
    "    task-specific orginal space, and uses this mix to step the environment in the original action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, scaler, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = scaler\n",
    "        self.weight = phi\n",
    "        \n",
    "        self.syn_act_space = self.pca.components_.shape[0]\n",
    "        self.no_syn_act_space = env.action_space.shape[0]\n",
    "        self.full_act_space = self.syn_act_space + self.no_syn_act_space\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.full_act_space,),dtype=np.float32)\n",
    "    def action(self, act):\n",
    "        syn_action = act[:self.syn_act_space]\n",
    "        no_syn_action = act[self.syn_act_space:]\n",
    "        \n",
    "        syn_action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([syn_action])))[0]\n",
    "        final_action = self.weight * syn_action + (1 - self.weight) * no_syn_action\n",
    "        \n",
    "        return final_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d80be",
   "metadata": {},
   "source": [
    "Additionally, we initialize a `SynergyWrapper`, which implements a variation of the above policy architecture that only leverages task-general synergistic activations (i.e., a more efficient implementation for handling the case where $\\varphi=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df901de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynergyWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    gym.ActionWrapper that reformulates the action space as the synergy space and inverse transforms\n",
    "    synergy-exploiting actions back into the original muscle activation space.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, ica, pca, phi):\n",
    "        super().__init__(env)\n",
    "        self.ica = ica\n",
    "        self.pca = pca\n",
    "        self.scaler = phi\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1., high=1., shape=(self.pca.components_.shape[0],),dtype=np.float32)\n",
    "    \n",
    "    def action(self, act):\n",
    "        action = self.pca.inverse_transform(self.ica.inverse_transform(self.scaler.inverse_transform([act])))\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070abb8",
   "metadata": {},
   "source": [
    "We proceed to train a policy with this architecture using `SAR_RL`. We find that a blend weight $\\varphi$=0.66 between the synergistic (task-general) and nonsynergistic (task-specific) activations works best in practice, though this should also be considered a hyperparameter that may require optimization for specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71fbeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAR_RL(env_name, policy_name, timesteps, seed, ica, pca, normalizer, phi=.66, syn_nosyn=True):\n",
    "    \"\"\"\n",
    "    Trains a policy using sb3 implementation of SAC + SynNoSynWrapper.\n",
    "    \n",
    "    env_name: str; name of gym env.\n",
    "    policy_name: str; choose unique identifier of this policy\n",
    "    timesteps: int; how long you want to train your policy for\n",
    "    seed: str (not int); relevant if you want to train multiple policies with the same params\n",
    "    ica: the ICA object\n",
    "    pca: the PCA object\n",
    "    normalizer: the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    \"\"\"\n",
    "    if syn_nosyn:\n",
    "        env = SynNoSynWrapper(gym.make(env_name), ica, pca, normalizer, phi)\n",
    "    else:\n",
    "        env = SynergyWrapper(gym.make(env_name), ica, pca, normalizer)\n",
    "    env = Monitor(env)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    net_shape = [400, 300]\n",
    "    policy_kwargs = dict(net_arch=dict(pi=net_shape, qf=net_shape))\n",
    "    \n",
    "    model = SAC('MlpPolicy', env, learning_rate=linear_schedule(.001), buffer_size=int(3e5),\n",
    "            learning_starts=5000, batch_size=256, tau=.02, gamma=.98, train_freq=(1, \"episode\"),\n",
    "            gradient_steps=-1,policy_kwargs=policy_kwargs, verbose=1)\n",
    "    \n",
    "    succ_callback = SaveSuccesses(check_freq=1, env_name=env_name+'_'+seed, \n",
    "                             log_dir=f'{policy_name}_successes_{env_name}_{seed}')\n",
    "    \n",
    "    model.set_logger(configure(f'{policy_name}_results_{env_name}_{seed}'))\n",
    "    model.learn(total_timesteps=int(timesteps), callback=succ_callback, log_interval=4)\n",
    "    model.save(f\"{policy_name}_model_{env_name}_{seed}\")\n",
    "    env.save(f'{policy_name}_env_{env_name}_{seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d9302",
   "metadata": {},
   "source": [
    "# Full training example 1: SAR x physiological locomotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c4882",
   "metadata": {},
   "source": [
    "Now that we have defined the core functions for implementing SAR, we will now train a locomotion policy using MyoLegs. In this example:\n",
    "- a policy is trained on straight flat walking task for 1.5M steps (`myoLegWalk-v0`)\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- Variance accounted for (VAF) by N synergies is computed. Here, we use 20 synergies from the 80-dimensional leg muscle activations\n",
    "- SAR is used to train on:\n",
    "    - a hilly terrain walking task (`myoLegHillyTerrainWalk-v0`)\n",
    "    - an uneven terrain walking task (`myoLegRoughTerrainWalk-v0`)\n",
    "    - a stair climbing task (`myoLegStairTerrainWalk-v0`)\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc17b86",
   "metadata": {},
   "source": [
    "# Step 1.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `myoLegWalk-v0` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de557",
   "metadata": {},
   "source": [
    "## Option 1.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e29fc68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m    MyoSuite: A contact-rich simulation suite for musculoskeletal motor control\n",
      "        Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, Vikash Kumar\n",
      "        L4DC-2019 | https://sites.google.com/view/myosuite\n",
      "    \u001b[0m\n",
      "Using cpu device\n",
      "Logging to play_period_results_myoLegWalk-v0_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35       |\n",
      "|    ep_rew_mean     | 52.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 966      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 140      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.8     |\n",
      "|    ep_rew_mean     | 59.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 984      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 286      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.1     |\n",
      "|    ep_rew_mean     | 59.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 990      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 433      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.9     |\n",
      "|    ep_rew_mean     | 55.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 984      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 575      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36       |\n",
      "|    ep_rew_mean     | 57       |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 984      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 719      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.8     |\n",
      "|    ep_rew_mean     | 55.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 986      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 860      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.7     |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 989      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 999      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.7     |\n",
      "|    ep_rew_mean     | 54.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 302      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 1141     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -148     |\n",
      "|    critic_loss     | 491      |\n",
      "|    ent_coef        | 0.912    |\n",
      "|    ent_coef_loss   | -11.6    |\n",
      "|    learning_rate   | 0.000999 |\n",
      "|    n_updates       | 110      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.8     |\n",
      "|    ep_rew_mean     | 53.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1288     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -260     |\n",
      "|    critic_loss     | 505      |\n",
      "|    ent_coef        | 0.798    |\n",
      "|    ent_coef_loss   | -25.8    |\n",
      "|    learning_rate   | 0.000999 |\n",
      "|    n_updates       | 255      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36       |\n",
      "|    ep_rew_mean     | 52.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1439     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 348      |\n",
      "|    ent_coef        | 0.697    |\n",
      "|    ent_coef_loss   | -38.2    |\n",
      "|    learning_rate   | 0.000999 |\n",
      "|    n_updates       | 402      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.3     |\n",
      "|    ep_rew_mean     | 55.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 108      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 1599     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 105      |\n",
      "|    ent_coef        | 0.604    |\n",
      "|    ent_coef_loss   | -52.6    |\n",
      "|    learning_rate   | 0.000999 |\n",
      "|    n_updates       | 560      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.1     |\n",
      "|    ep_rew_mean     | 54.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 1732     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 190      |\n",
      "|    ent_coef        | 0.53     |\n",
      "|    ent_coef_loss   | -66.2    |\n",
      "|    learning_rate   | 0.000999 |\n",
      "|    n_updates       | 695      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.1     |\n",
      "|    ep_rew_mean     | 53.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 1875     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -550     |\n",
      "|    critic_loss     | 171      |\n",
      "|    ent_coef        | 0.467    |\n",
      "|    ent_coef_loss   | -76.6    |\n",
      "|    learning_rate   | 0.000999 |\n",
      "|    n_updates       | 835      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmyoLegWalk-v0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mplay_period\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.5e6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env_name, policy_name, timesteps, seed)\u001b[39m\n\u001b[32m     22\u001b[39m succ_callback = SaveSuccesses(check_freq=\u001b[32m1\u001b[39m, env_name=env_name+\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m+seed, \n\u001b[32m     23\u001b[39m                          log_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_successes_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m model.set_logger(configure(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43msucc_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m model.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m env.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_env_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/sac/sac.py:307\u001b[39m, in \u001b[36mSAC.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[32m    300\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    306\u001b[39m ) -> SelfSAC:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    345\u001b[39m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m callback.on_training_end()\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/sac/sac.py:222\u001b[39m, in \u001b[36mSAC.train\u001b[39m\u001b[34m(self, gradient_steps, batch_size)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.actor.reset_noise()\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Action by the current actor for the sampled state\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m actions_pi, log_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m log_prob = log_prob.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    225\u001b[39m ent_coef_loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/sac/policies.py:175\u001b[39m, in \u001b[36mActor.action_log_prob\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    173\u001b[39m mean_actions, log_std, kwargs = \u001b[38;5;28mself\u001b[39m.get_action_dist_params(obs)\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# return action and associated log prob\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob_from_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:258\u001b[39m, in \u001b[36mSquashedDiagGaussianDistribution.log_prob_from_params\u001b[39m\u001b[34m(self, mean_actions, log_std)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob_from_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean_actions: th.Tensor, log_std: th.Tensor) -> \u001b[38;5;28mtuple\u001b[39m[th.Tensor, th.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions_from_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m     log_prob = \u001b[38;5;28mself\u001b[39m.log_prob(action, \u001b[38;5;28mself\u001b[39m.gaussian_actions)\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m action, log_prob\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:190\u001b[39m, in \u001b[36mDiagGaussianDistribution.actions_from_params\u001b[39m\u001b[34m(self, mean_actions, log_std, deterministic)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mactions_from_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean_actions: th.Tensor, log_std: th.Tensor, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# Update the proba distribution\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_actions(deterministic=deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:224\u001b[39m, in \u001b[36mSquashedDiagGaussianDistribution.proba_distribution\u001b[39m\u001b[34m(self, mean_actions, log_std)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproba_distribution\u001b[39m(\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfSquashedDiagGaussianDistribution, mean_actions: th.Tensor, log_std: th.Tensor\n\u001b[32m    223\u001b[39m ) -> SelfSquashedDiagGaussianDistribution:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:164\u001b[39m, in \u001b[36mDiagGaussianDistribution.proba_distribution\u001b[39m\u001b[34m(self, mean_actions, log_std)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[32m    158\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m \u001b[33;03m:return:\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m action_std = th.ones_like(mean_actions) * log_std.exp()\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28mself\u001b[39m.distribution = \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/distributions/normal.py:59\u001b[39m, in \u001b[36mNormal.__init__\u001b[39m\u001b[34m(self, loc, scale, validate_args)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     58\u001b[39m     batch_shape = \u001b[38;5;28mself\u001b[39m.loc.size()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/distributions/distribution.py:70\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train('myoLegWalk-v0', 'play_period', 1.5e6, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ffb8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始创建环境...\n",
      "\u001b[43m\u001b[30mWarning: Unused kwargs found: {'render_mode': 'rgb_array', 'width': 1920, 'height': 1080}\u001b[0m\n",
      "环境创建成功\n",
      "加载模型: play_period\n",
      "发生错误: [Errno 2] No such file or directory: 'play_period.zip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/cedriclau/Desktop/SAR/myosuite-main/docs/source/tutorials/SAR/SAR_tutorial_utils.py\", line 59, in get_vid\n",
      "    model = SAC.load(f\"{name}\")\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/base_class.py\", line 681, in load\n",
      "    data, params, pytorch_variables = load_from_zip_file(\n",
      "                                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/save_util.py\", line 403, in load_from_zip_file\n",
      "    file = open_path(load_path, \"r\", verbose=verbose, suffix=\"zip\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/functools.py\", line 912, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/save_util.py\", line 240, in open_path_str\n",
      "    return open_path_pathlib(pathlib.Path(path), mode, verbose, suffix)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/save_util.py\", line 291, in open_path_pathlib\n",
      "    return open_path_pathlib(path, mode, verbose, suffix)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/save_util.py\", line 272, in open_path_pathlib\n",
      "    raise error\n",
      "  File \"/opt/miniconda3/lib/python3.12/site-packages/stable_baselines3/common/save_util.py\", line 264, in open_path_pathlib\n",
      "    return open_path(path.open(\"rb\"), mode, verbose, suffix)\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.12/pathlib.py\", line 1013, in open\n",
      "    return io.open(self, mode, buffering, encoding, errors, newline)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'play_period.zip'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show_video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m video_name = \u001b[33m'\u001b[39m\u001b[33mwalk_play_period_video\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m get_vid(name=\u001b[33m'\u001b[39m\u001b[33mplay_period\u001b[39m\u001b[33m'\u001b[39m, env_name=\u001b[33m'\u001b[39m\u001b[33mmyoLegWalk-v0\u001b[39m\u001b[33m'\u001b[39m, seed=\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m, episodes=\u001b[32m10\u001b[39m, video_name=video_name)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mshow_video\u001b[49m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'show_video' is not defined"
     ]
    }
   ],
   "source": [
    "video_name = 'walk_play_period_video'\n",
    "get_vid(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358d350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "muscle_data = get_activations(name='play_period', env_name='myoLegWalk-v0', seed='0', episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee90ed-73c9-4b02-ba56-488d539c4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "muscle_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2512d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc6df4",
   "metadata": {},
   "source": [
    "## Option 1.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d76eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica,pca,normalizer = load_locomotion_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e199df",
   "metadata": {},
   "source": [
    "# Step 1.2: Train on unseen terrain with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c91b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myoLegRoughTerrainWalk-v0 selected for training.\n"
     ]
    }
   ],
   "source": [
    "# randomly choose terrain to learn, or manually choose one\n",
    "new_terrain = np.random.choice(['Hilly', 'Stair', 'Rough'])\n",
    "# new_terrain = ...\n",
    "print(f'myoLeg{new_terrain}TerrainWalk-v0 selected for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e101f52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to SAR-RL_results_myoLegRoughTerrainWalk-v0_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.8     |\n",
      "|    ep_rew_mean     | 99.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 799      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 155      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 97.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 804      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 290      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.8     |\n",
      "|    ep_rew_mean     | 85.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 808      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 441      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.6     |\n",
      "|    ep_rew_mean     | 98       |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 809      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 602      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.1     |\n",
      "|    ep_rew_mean     | 97.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 811      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 743      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.1     |\n",
      "|    ep_rew_mean     | 94.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 811      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 891      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 94.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 802      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1028     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 94.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 799      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1164     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 97.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 802      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1317     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37       |\n",
      "|    ep_rew_mean     | 99.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 804      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1480     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.8     |\n",
      "|    ep_rew_mean     | 97.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 802      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1621     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37       |\n",
      "|    ep_rew_mean     | 98.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 802      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1775     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.8     |\n",
      "|    ep_rew_mean     | 97       |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 802      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1914     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.8     |\n",
      "|    ep_rew_mean     | 97.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 804      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2060     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.8     |\n",
      "|    ep_rew_mean     | 95.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 805      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2209     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.5     |\n",
      "|    ep_rew_mean     | 94.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 804      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2338     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 93.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 803      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2476     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 92.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 804      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2624     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.5     |\n",
      "|    ep_rew_mean     | 91.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 805      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2776     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 91.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 805      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2928     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 90.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 807      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 3081     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 91.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 807      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 3221     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 90.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 808      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 3377     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 91.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 808      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 3526     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 90.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 809      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 3660     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 88.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 810      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 3798     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.5     |\n",
      "|    ep_rew_mean     | 88.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 810      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 3944     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 91.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 811      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4110     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 88.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 811      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4245     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.5     |\n",
      "|    ep_rew_mean     | 88.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 812      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4390     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 89.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 812      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4548     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.5     |\n",
      "|    ep_rew_mean     | 88.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 812      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4683     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 88.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 812      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.5     |\n",
      "|    ep_rew_mean     | 88.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 812      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4970     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 88.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 598      |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 5122     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -42.4    |\n",
      "|    critic_loss     | 267      |\n",
      "|    ent_coef        | 0.928    |\n",
      "|    ent_coef_loss   | -2.49    |\n",
      "|    learning_rate   | 0.000797 |\n",
      "|    n_updates       | 116      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.6     |\n",
      "|    ep_rew_mean     | 92       |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 428      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 5286     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -76.7    |\n",
      "|    critic_loss     | 76.8     |\n",
      "|    ent_coef        | 0.818    |\n",
      "|    ent_coef_loss   | -6.19    |\n",
      "|    learning_rate   | 0.00079  |\n",
      "|    n_updates       | 278      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 92       |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 351      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 5414     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -101     |\n",
      "|    critic_loss     | 84.6     |\n",
      "|    ent_coef        | 0.737    |\n",
      "|    ent_coef_loss   | -9.16    |\n",
      "|    learning_rate   | 0.000785 |\n",
      "|    n_updates       | 413      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 92.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 308      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 5535     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -118     |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    ent_coef        | 0.674    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.00078  |\n",
      "|    n_updates       | 533      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 91.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 270      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 5676     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 37.9     |\n",
      "|    ent_coef        | 0.615    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.000775 |\n",
      "|    n_updates       | 663      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36       |\n",
      "|    ep_rew_mean     | 90.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 236      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 5813     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    ent_coef        | 0.557    |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.000769 |\n",
      "|    n_updates       | 812      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | 87.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 5975     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -168     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.505    |\n",
      "|    ent_coef_loss   | -16.5    |\n",
      "|    learning_rate   | 0.000763 |\n",
      "|    n_updates       | 961      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.3     |\n",
      "|    ep_rew_mean     | 90.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 6110     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -179     |\n",
      "|    critic_loss     | 31.9     |\n",
      "|    ent_coef        | 0.458    |\n",
      "|    ent_coef_loss   | -17.7    |\n",
      "|    learning_rate   | 0.000757 |\n",
      "|    n_updates       | 1114     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 95.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 6247     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 45.1     |\n",
      "|    ent_coef        | 0.422    |\n",
      "|    ent_coef_loss   | -19.8    |\n",
      "|    learning_rate   | 0.000752 |\n",
      "|    n_updates       | 1239     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.2     |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 6401     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    ent_coef        | 0.384    |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.000746 |\n",
      "|    n_updates       | 1391     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 6600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    ent_coef        | 0.342    |\n",
      "|    ent_coef_loss   | -21.7    |\n",
      "|    learning_rate   | 0.000738 |\n",
      "|    n_updates       | 1587     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.7     |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 6753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 31.2     |\n",
      "|    ent_coef        | 0.31     |\n",
      "|    ent_coef_loss   | -22      |\n",
      "|    learning_rate   | 0.000732 |\n",
      "|    n_updates       | 1742     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.9     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 6907     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -210     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    ent_coef        | 0.284    |\n",
      "|    ent_coef_loss   | -23      |\n",
      "|    learning_rate   | 0.000725 |\n",
      "|    n_updates       | 1898     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.8     |\n",
      "|    ep_rew_mean     | 112      |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 7061     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -208     |\n",
      "|    critic_loss     | 23.4     |\n",
      "|    ent_coef        | 0.259    |\n",
      "|    ent_coef_loss   | -25.1    |\n",
      "|    learning_rate   | 0.000719 |\n",
      "|    n_updates       | 2055     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37       |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 7222     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -201     |\n",
      "|    critic_loss     | 21       |\n",
      "|    ent_coef        | 0.236    |\n",
      "|    ent_coef_loss   | -26.9    |\n",
      "|    learning_rate   | 0.000713 |\n",
      "|    n_updates       | 2210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.2     |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 7380     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    ent_coef        | 0.215    |\n",
      "|    ent_coef_loss   | -27.2    |\n",
      "|    learning_rate   | 0.000706 |\n",
      "|    n_updates       | 2372     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.5     |\n",
      "|    ep_rew_mean     | 134      |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 7552     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -191     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.0007   |\n",
      "|    n_updates       | 2539     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38       |\n",
      "|    ep_rew_mean     | 142      |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 117      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 7741     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -187     |\n",
      "|    critic_loss     | 15.3     |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | -27.9    |\n",
      "|    learning_rate   | 0.000693 |\n",
      "|    n_updates       | 2714     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.3     |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 112      |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 7943     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -174     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.158    |\n",
      "|    ent_coef_loss   | -28      |\n",
      "|    learning_rate   | 0.000684 |\n",
      "|    n_updates       | 2926     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.8     |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 108      |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 8126     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -26      |\n",
      "|    learning_rate   | 0.000676 |\n",
      "|    n_updates       | 3118     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 39.3     |\n",
      "|    ep_rew_mean     | 159      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 105      |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 8321     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.131    |\n",
      "|    ent_coef_loss   | -24.2    |\n",
      "|    learning_rate   | 0.000669 |\n",
      "|    n_updates       | 3299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40.1     |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 8554     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -145     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.118    |\n",
      "|    ent_coef_loss   | -23.7    |\n",
      "|    learning_rate   | 0.00066  |\n",
      "|    n_updates       | 3531     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.4     |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 8820     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.106    |\n",
      "|    ent_coef_loss   | -20.5    |\n",
      "|    learning_rate   | 0.00065  |\n",
      "|    n_updates       | 3789     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42.2     |\n",
      "|    ep_rew_mean     | 177      |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 9036     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -130     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.097    |\n",
      "|    ent_coef_loss   | -18.5    |\n",
      "|    learning_rate   | 0.00064  |\n",
      "|    n_updates       | 4019     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42.6     |\n",
      "|    ep_rew_mean     | 183      |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 9233     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -123     |\n",
      "|    critic_loss     | 11       |\n",
      "|    ent_coef        | 0.0903   |\n",
      "|    ent_coef_loss   | -18.1    |\n",
      "|    learning_rate   | 0.000633 |\n",
      "|    n_updates       | 4216     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.3     |\n",
      "|    ep_rew_mean     | 191      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 9451     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -117     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0838   |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.000624 |\n",
      "|    n_updates       | 4428     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.9     |\n",
      "|    ep_rew_mean     | 191      |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 9671     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -115     |\n",
      "|    critic_loss     | 9.99     |\n",
      "|    ent_coef        | 0.078    |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.000615 |\n",
      "|    n_updates       | 4654     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 44.6     |\n",
      "|    ep_rew_mean     | 196      |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 87       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 9876     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -110     |\n",
      "|    critic_loss     | 9.39     |\n",
      "|    ent_coef        | 0.074    |\n",
      "|    ent_coef_loss   | -11.7    |\n",
      "|    learning_rate   | 0.000608 |\n",
      "|    n_updates       | 4835     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45.6     |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 10092    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -107     |\n",
      "|    critic_loss     | 9.72     |\n",
      "|    ent_coef        | 0.0698   |\n",
      "|    ent_coef_loss   | -11.1    |\n",
      "|    learning_rate   | 0.000599 |\n",
      "|    n_updates       | 5046     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46.7     |\n",
      "|    ep_rew_mean     | 212      |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 10342    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -101     |\n",
      "|    critic_loss     | 9.23     |\n",
      "|    ent_coef        | 0.0658   |\n",
      "|    ent_coef_loss   | -9.55    |\n",
      "|    learning_rate   | 0.000589 |\n",
      "|    n_updates       | 5297     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.4     |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 10650    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.2    |\n",
      "|    critic_loss     | 8.32     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | -7.42    |\n",
      "|    learning_rate   | 0.000577 |\n",
      "|    n_updates       | 5613     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.1     |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 10988    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -96.5    |\n",
      "|    critic_loss     | 9.31     |\n",
      "|    ent_coef        | 0.0584   |\n",
      "|    ent_coef_loss   | -3.74    |\n",
      "|    learning_rate   | 0.000564 |\n",
      "|    n_updates       | 5942     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.9     |\n",
      "|    ep_rew_mean     | 228      |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 78       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -95.3    |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.057    |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 0.000554 |\n",
      "|    n_updates       | 6183     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 51.4     |\n",
      "|    ep_rew_mean     | 227      |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 77       |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 11384    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94      |\n",
      "|    critic_loss     | 9.45     |\n",
      "|    ent_coef        | 0.0562   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.000546 |\n",
      "|    n_updates       | 6369     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 52       |\n",
      "|    ep_rew_mean     | 228      |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 11604    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.7    |\n",
      "|    critic_loss     | 9.8      |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.000538 |\n",
      "|    n_updates       | 6580     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 52.5     |\n",
      "|    ep_rew_mean     | 236      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 11846    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.8    |\n",
      "|    critic_loss     | 9.91     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.000529 |\n",
      "|    n_updates       | 6809     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.2     |\n",
      "|    ep_rew_mean     | 242      |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 12075    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -96.9    |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0542   |\n",
      "|    ent_coef_loss   | -0.599   |\n",
      "|    learning_rate   | 0.000519 |\n",
      "|    n_updates       | 7057     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.8     |\n",
      "|    ep_rew_mean     | 246      |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 12290    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.2    |\n",
      "|    critic_loss     | 8.44     |\n",
      "|    ent_coef        | 0.0531   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.00051  |\n",
      "|    n_updates       | 7269     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.3     |\n",
      "|    ep_rew_mean     | 249      |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 72       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 12492    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -93.6    |\n",
      "|    critic_loss     | 8.51     |\n",
      "|    ent_coef        | 0.053    |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.000502 |\n",
      "|    n_updates       | 7469     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 55.3     |\n",
      "|    ep_rew_mean     | 250      |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 12754    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89.3    |\n",
      "|    critic_loss     | 7.88     |\n",
      "|    ent_coef        | 0.0531   |\n",
      "|    ent_coef_loss   | -0.831   |\n",
      "|    learning_rate   | 0.000493 |\n",
      "|    n_updates       | 7716     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 57.5     |\n",
      "|    ep_rew_mean     | 257      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 13130    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.8    |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.0521   |\n",
      "|    ent_coef_loss   | -0.61    |\n",
      "|    learning_rate   | 0.000478 |\n",
      "|    n_updates       | 8090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58.2     |\n",
      "|    ep_rew_mean     | 258      |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 13376    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -87.6    |\n",
      "|    critic_loss     | 8.89     |\n",
      "|    ent_coef        | 0.0517   |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.000467 |\n",
      "|    n_updates       | 8358     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.6     |\n",
      "|    ep_rew_mean     | 258      |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 68       |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 13700    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.1    |\n",
      "|    critic_loss     | 8.16     |\n",
      "|    ent_coef        | 0.0522   |\n",
      "|    ent_coef_loss   | 0.9      |\n",
      "|    learning_rate   | 0.000455 |\n",
      "|    n_updates       | 8655     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60.1     |\n",
      "|    ep_rew_mean     | 263      |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 13955    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -87.4    |\n",
      "|    critic_loss     | 8.13     |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | 0.0108   |\n",
      "|    learning_rate   | 0.000444 |\n",
      "|    n_updates       | 8942     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 61.4     |\n",
      "|    ep_rew_mean     | 265      |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 66       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 14264    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89.6    |\n",
      "|    critic_loss     | 8        |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | 0.514    |\n",
      "|    learning_rate   | 0.000432 |\n",
      "|    n_updates       | 9224     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 62.2     |\n",
      "|    ep_rew_mean     | 267      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 14540    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89.1    |\n",
      "|    critic_loss     | 7.54     |\n",
      "|    ent_coef        | 0.052    |\n",
      "|    ent_coef_loss   | -0.709   |\n",
      "|    learning_rate   | 0.000421 |\n",
      "|    n_updates       | 9508     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 62       |\n",
      "|    ep_rew_mean     | 269      |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 65       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 14757    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -86.7    |\n",
      "|    critic_loss     | 7.79     |\n",
      "|    ent_coef        | 0.0511   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.000411 |\n",
      "|    n_updates       | 9751     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 61.6     |\n",
      "|    ep_rew_mean     | 270      |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 14978    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.2    |\n",
      "|    critic_loss     | 8.15     |\n",
      "|    ent_coef        | 0.0504   |\n",
      "|    ent_coef_loss   | 0.00664  |\n",
      "|    learning_rate   | 0.000403 |\n",
      "|    n_updates       | 9947     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 62.1     |\n",
      "|    ep_rew_mean     | 270      |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 64       |\n",
      "|    time_elapsed    | 237      |\n",
      "|    total_timesteps | 15249    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.2    |\n",
      "|    critic_loss     | 8.39     |\n",
      "|    ent_coef        | 0.0506   |\n",
      "|    ent_coef_loss   | -0.708   |\n",
      "|    learning_rate   | 0.000393 |\n",
      "|    n_updates       | 10202    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 63.6     |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 15593    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -86      |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | -0.106   |\n",
      "|    learning_rate   | 0.000379 |\n",
      "|    n_updates       | 10558    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.5     |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 15904    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.2    |\n",
      "|    critic_loss     | 8.95     |\n",
      "|    ent_coef        | 0.0516   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.000366 |\n",
      "|    n_updates       | 10878    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.5     |\n",
      "|    ep_rew_mean     | 291      |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 62       |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 16124    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.4    |\n",
      "|    critic_loss     | 8.65     |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | 0.111    |\n",
      "|    learning_rate   | 0.000357 |\n",
      "|    n_updates       | 11106    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.6     |\n",
      "|    ep_rew_mean     | 294      |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 16339    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -86      |\n",
      "|    critic_loss     | 8.59     |\n",
      "|    ent_coef        | 0.054    |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.000348 |\n",
      "|    n_updates       | 11320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.3     |\n",
      "|    ep_rew_mean     | 294      |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 16727    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -90.3    |\n",
      "|    critic_loss     | 9.33     |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.000336 |\n",
      "|    n_updates       | 11631    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67.9     |\n",
      "|    ep_rew_mean     | 286      |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 17130    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -90.9    |\n",
      "|    critic_loss     | 9.24     |\n",
      "|    ent_coef        | 0.0563   |\n",
      "|    ent_coef_loss   | 0.801    |\n",
      "|    learning_rate   | 0.000319 |\n",
      "|    n_updates       | 12064    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69       |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 17554    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -91.7    |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0583   |\n",
      "|    ent_coef_loss   | 0.225    |\n",
      "|    learning_rate   | 0.000303 |\n",
      "|    n_updates       | 12459    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.2     |\n",
      "|    ep_rew_mean     | 289      |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 59       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 17907    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -90.4    |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0586   |\n",
      "|    ent_coef_loss   | 0.182    |\n",
      "|    learning_rate   | 0.000287 |\n",
      "|    n_updates       | 12858    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70       |\n",
      "|    ep_rew_mean     | 291      |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 59       |\n",
      "|    time_elapsed    | 308      |\n",
      "|    total_timesteps | 18205    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -92.5    |\n",
      "|    critic_loss     | 9.59     |\n",
      "|    ent_coef        | 0.0583   |\n",
      "|    ent_coef_loss   | 0.301    |\n",
      "|    learning_rate   | 0.000275 |\n",
      "|    n_updates       | 13156    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.5     |\n",
      "|    ep_rew_mean     | 297      |\n",
      "| time/              |          |\n",
      "|    episodes        | 372      |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 18539    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.1    |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -0.123   |\n",
      "|    learning_rate   | 0.000262 |\n",
      "|    n_updates       | 13489    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72       |\n",
      "|    ep_rew_mean     | 299      |\n",
      "| time/              |          |\n",
      "|    episodes        | 376      |\n",
      "|    fps             | 58       |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 18809    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -93      |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0571   |\n",
      "|    ent_coef_loss   | 0.0329   |\n",
      "|    learning_rate   | 0.000251 |\n",
      "|    n_updates       | 13766    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.3     |\n",
      "|    ep_rew_mean     | 301      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 19072    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.1    |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0575   |\n",
      "|    ent_coef_loss   | 0.149    |\n",
      "|    learning_rate   | 0.000239 |\n",
      "|    n_updates       | 14044    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.3     |\n",
      "|    ep_rew_mean     | 302      |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 19309    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -93      |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0571   |\n",
      "|    ent_coef_loss   | -0.731   |\n",
      "|    learning_rate   | 0.00023  |\n",
      "|    n_updates       | 14285    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.9     |\n",
      "|    ep_rew_mean     | 303      |\n",
      "| time/              |          |\n",
      "|    episodes        | 388      |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 19580    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -93.1    |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0564   |\n",
      "|    ent_coef_loss   | -0.794   |\n",
      "|    learning_rate   | 0.00022  |\n",
      "|    n_updates       | 14530    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.5     |\n",
      "|    ep_rew_mean     | 310      |\n",
      "| time/              |          |\n",
      "|    episodes        | 392      |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 19941    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.5    |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    ent_coef        | 0.0558   |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.000206 |\n",
      "|    n_updates       | 14885    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.8     |\n",
      "|    ep_rew_mean     | 305      |\n",
      "| time/              |          |\n",
      "|    episodes        | 396      |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 20236    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -90.2    |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0566   |\n",
      "|    ent_coef_loss   | 0.758    |\n",
      "|    learning_rate   | 0.000193 |\n",
      "|    n_updates       | 15200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.5     |\n",
      "|    ep_rew_mean     | 298      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 363      |\n",
      "|    total_timesteps | 20475    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.2    |\n",
      "|    critic_loss     | 11       |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.000183 |\n",
      "|    n_updates       | 15445    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.3     |\n",
      "|    ep_rew_mean     | 298      |\n",
      "| time/              |          |\n",
      "|    episodes        | 404      |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total_timesteps | 20811    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.8    |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | -0.0552  |\n",
      "|    learning_rate   | 0.000171 |\n",
      "|    n_updates       | 15756    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.3     |\n",
      "|    ep_rew_mean     | 298      |\n",
      "| time/              |          |\n",
      "|    episodes        | 408      |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 376      |\n",
      "|    total_timesteps | 21027    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.6    |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.057    |\n",
      "|    ent_coef_loss   | -0.984   |\n",
      "|    learning_rate   | 0.000161 |\n",
      "|    n_updates       | 16005    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73       |\n",
      "|    ep_rew_mean     | 295      |\n",
      "| time/              |          |\n",
      "|    episodes        | 412      |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 21258    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.9    |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0563   |\n",
      "|    ent_coef_loss   | -0.492   |\n",
      "|    learning_rate   | 0.000152 |\n",
      "|    n_updates       | 16218    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.1     |\n",
      "|    ep_rew_mean     | 294      |\n",
      "| time/              |          |\n",
      "|    episodes        | 416      |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 391      |\n",
      "|    total_timesteps | 21677    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -86.7    |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0559   |\n",
      "|    ent_coef_loss   | -0.984   |\n",
      "|    learning_rate   | 0.000138 |\n",
      "|    n_updates       | 16591    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 292      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 21924    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.1    |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.055    |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.000125 |\n",
      "|    n_updates       | 16893    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.4     |\n",
      "|    ep_rew_mean     | 292      |\n",
      "| time/              |          |\n",
      "|    episodes        | 424      |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 403      |\n",
      "|    total_timesteps | 22098    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.4    |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0546   |\n",
      "|    ent_coef_loss   | 0.37     |\n",
      "|    learning_rate   | 0.000118 |\n",
      "|    n_updates       | 17082    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.9     |\n",
      "|    ep_rew_mean     | 293      |\n",
      "| time/              |          |\n",
      "|    episodes        | 428      |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 407      |\n",
      "|    total_timesteps | 22272    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.4    |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | 0.389    |\n",
      "|    learning_rate   | 0.000111 |\n",
      "|    n_updates       | 17260    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72       |\n",
      "|    ep_rew_mean     | 294      |\n",
      "| time/              |          |\n",
      "|    episodes        | 432      |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 411      |\n",
      "|    total_timesteps | 22451    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.5    |\n",
      "|    critic_loss     | 13.3     |\n",
      "|    ent_coef        | 0.0546   |\n",
      "|    ent_coef_loss   | -0.186   |\n",
      "|    learning_rate   | 0.000104 |\n",
      "|    n_updates       | 17433    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.8     |\n",
      "|    ep_rew_mean     | 287      |\n",
      "| time/              |          |\n",
      "|    episodes        | 436      |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 418      |\n",
      "|    total_timesteps | 22778    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.9    |\n",
      "|    critic_loss     | 13.8     |\n",
      "|    ent_coef        | 0.0546   |\n",
      "|    ent_coef_loss   | 0.09     |\n",
      "|    learning_rate   | 9.23e-05 |\n",
      "|    n_updates       | 17723    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.6     |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 427      |\n",
      "|    total_timesteps | 23165    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.5    |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | 0.465    |\n",
      "|    learning_rate   | 7.7e-05  |\n",
      "|    n_updates       | 18105    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.6     |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 444      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 435      |\n",
      "|    total_timesteps | 23487    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -80.4    |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | -0.397   |\n",
      "|    learning_rate   | 6.41e-05 |\n",
      "|    n_updates       | 18428    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74       |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 448      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 441      |\n",
      "|    total_timesteps | 23743    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -79.3    |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | -0.768   |\n",
      "|    learning_rate   | 5.29e-05 |\n",
      "|    n_updates       | 18708    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.5     |\n",
      "|    ep_rew_mean     | 282      |\n",
      "| time/              |          |\n",
      "|    episodes        | 452      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 447      |\n",
      "|    total_timesteps | 23974    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.1    |\n",
      "|    critic_loss     | 15.4     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -0.931   |\n",
      "|    learning_rate   | 4.31e-05 |\n",
      "|    n_updates       | 18952    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.5     |\n",
      "|    ep_rew_mean     | 289      |\n",
      "| time/              |          |\n",
      "|    episodes        | 456      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 452      |\n",
      "|    total_timesteps | 24183    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.6    |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.0237  |\n",
      "|    learning_rate   | 3.47e-05 |\n",
      "|    n_updates       | 19163    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.2     |\n",
      "|    ep_rew_mean     | 293      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 456      |\n",
      "|    total_timesteps | 24370    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.4    |\n",
      "|    critic_loss     | 17       |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.247   |\n",
      "|    learning_rate   | 2.7e-05  |\n",
      "|    n_updates       | 19355    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.6     |\n",
      "|    ep_rew_mean     | 288      |\n",
      "| time/              |          |\n",
      "|    episodes        | 464      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 460      |\n",
      "|    total_timesteps | 24565    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -79.2    |\n",
      "|    critic_loss     | 16.6     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | 0.142    |\n",
      "|    learning_rate   | 1.98e-05 |\n",
      "|    n_updates       | 19536    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.3     |\n",
      "|    ep_rew_mean     | 286      |\n",
      "| time/              |          |\n",
      "|    episodes        | 468      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 465      |\n",
      "|    total_timesteps | 24740    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.5    |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.482   |\n",
      "|    learning_rate   | 1.23e-05 |\n",
      "|    n_updates       | 19722    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.2     |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 472      |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 470      |\n",
      "|    total_timesteps | 24959    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -74.9    |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 4.08e-06 |\n",
      "|    n_updates       | 19928    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "SAR_RL(env_name=f'myoLeg{new_terrain}TerrainWalk-v0', policy_name='SAR-RL', timesteps=2.5e4, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66, syn_nosyn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6f812b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_vid() got an unexpected keyword argument 'determ'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m video_name = \u001b[33m'\u001b[39m\u001b[33mwalk_SAR-RL_video\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mget_vid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSAR-RL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmyoLeg\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnew_terrain\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mTerrainWalk-v0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeterm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mica\u001b[49m\u001b[43m=\u001b[49m\u001b[43mica\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m.66\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_sar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn_nosyn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m show_video(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: get_vid() got an unexpected keyword argument 'determ'"
     ]
    }
   ],
   "source": [
    "video_name = 'walk_SAR-RL_video'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name=f'myoLeg{new_terrain}TerrainWalk-v0', seed='0', episodes=5, video_name=video_name, \n",
    "        determ=False, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=False)\n",
    "\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c68a0a-eede-4505-b800-5c0ccdd831bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.make(f'myoLeg{new_terrain}TerrainWalk-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10cb1e",
   "metadata": {},
   "source": [
    "# Step 1.3 (optional): Train on unseen terrain with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2ad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(f'myoLeg{new_terrain}TerrainWalk-v0', 'RL-E2E', 4e6, '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16629ae9",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ac0f3",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9424151",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='locomotion', terrain=new_terrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf790b",
   "metadata": {},
   "source": [
    "# Full training example 2: SAR x physiological manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cbb1e",
   "metadata": {},
   "source": [
    "Now, we turn to using synergies for manipulation rather than locomotion. In this example:\n",
    "- a policy is trained on an eight-object reorientation task, `Reorient8-v0`.\n",
    "- after training, we collect muscle activation data from policy rollouts\n",
    "- variance accounted for (VAF) by N synergies is computed. Here, we select N where VAF > 0.8.\n",
    "- SAR is computed at this VAF threshold.\n",
    "- SAR is used to train on a 100-object reorientation task, `Reorient100-v0`.\n",
    "\n",
    "<strong>Note: we also provide the option of using pretrained policies/representations at some of the above steps. Run the code below depending on your preferences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1963d71",
   "metadata": {},
   "source": [
    "# Step 2.1\n",
    "\n",
    "First, we acquire our synergistic action representation. We provide two options for this:\n",
    "\n",
    "<strong>Option A: Get SAR from scratch (i.e., train on `Reorient8` as play period → get muscle activations → compute SAR).</strong>\n",
    "\n",
    "<strong>Option B: Use the precomputed SAR.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c960e8",
   "metadata": {},
   "source": [
    "## Option 2.1.A: Get SAR from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a policy is trained on an eight-object reorientation task, Reorient8-v0\n",
    "train(env_name='myoHandReorient8-v0', policy_name='play_period', timesteps=1e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f34571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after training, we collect muscle activation data from policy rollouts\n",
    "muscle_data = get_activations(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a109e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAF by N synergies is computed. Here, N is selected where VAF > 0.8\n",
    "syn_dict = find_synergies(muscle_data, plot=True)\n",
    "print(\"VAF by N synergies:\", syn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'play_period_vid'\n",
    "get_vid(name='play_period', env_name='myoHandReorient8-v0', seed='0', episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAR is computed at this VAF threshold\n",
    "ica,pca,normalizer = compute_SAR(muscle_data, 20, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e6fd8",
   "metadata": {},
   "source": [
    "## Option 2.1.B: Use precomputed SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592deab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO USE PRECOMPUTED SAR\n",
    "ica,pca,normalizer = load_manipulation_SAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be153658",
   "metadata": {},
   "source": [
    "# Step 2.2: Train on Reorient100 with synergies (SAR-RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf7de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SAR is used to train on a 100-object reorientation task, Reorient100-v0\n",
    "SAR_RL(env_name='myoHandReorient100-v0', policy_name='SAR-RL', timesteps=1.5e6, \n",
    "       seed='0', ica=ica, pca=pca, normalizer=normalizer, phi=.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'SAR_vid'\n",
    "\n",
    "get_vid(name='SAR-RL', env_name='myoHandReorient100-v0', seed='0', episodes=10, video_name=video_name, \n",
    "        determ=True, pca=pca, ica=ica, normalizer=normalizer, phi=.66, is_sar=True, syn_nosyn=True)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc00d6",
   "metadata": {},
   "source": [
    "# Step 2.3 (optional): Train on Reorient100 with end-to-end RL (RL-E2E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f27d1",
   "metadata": {},
   "source": [
    "We can compare the performance of (A) pretraining for 1M steps on `Reorient8-v0`, getting SAR, and training with SAR-RL on `Reorient100-v0` for 2M steps with (B) training for 3M steps using end-to-end RL ('RL-E2E')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d36ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(env_name='myoHandReorient100-v0', policy_name='RL-E2E', timesteps=2.5e6, seed='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d350e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and load a video of the trained policy\n",
    "\n",
    "video_name = 'RL+E2E_vid'\n",
    "get_vid('RL-E2E', 'myoHandReorient100-v0', '0', determ=False, episodes=10, video_name=video_name)\n",
    "show_video(f\"{video_name}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154c777",
   "metadata": {},
   "source": [
    "# Step 2.4: Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1717e5",
   "metadata": {},
   "source": [
    "For convenience, a function is defined to automatically plot the results of these runs (ensure the names and seeds are preserved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05215d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(experiment='manipulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7bc976",
   "metadata": {},
   "source": [
    "# Step 2.5 (optional): zero-shot testing manipulation policies on new unseen objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bba92d",
   "metadata": {},
   "source": [
    "We can optionally test these manipulation policies on unseen objects to determine their generalizability. Here, we implement two test\n",
    "environments.\n",
    "\n",
    "1. ReorientID-v0—1000 unseen objects generated by sampling from the same dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with the same kind of shapes as those seen in the training set.\n",
    "\n",
    "2. ReorientOOD-v0—1000 unseen objects generated by sampling from different dimensions as those for the Reorient100-v0 set. Intuitively, this environment contains new objects with different kinds of shapes as those seen in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba66889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_test(name, test_env_name, env_name='myoHandReorient100-v0', seed='0', determ=True, ica=None, \n",
    "                  pca=None, normalizer=None, phi=.66, episodes=500, is_sar=False, syn_nosyn=False):\n",
    "    \"\"\"\n",
    "    Check zero-shot performance of policies on the test environments.\n",
    "    \n",
    "    name: str; name of the policy to test\n",
    "    env_name: str; name of gym env the policy to test was trained on (Reorient100-v0).\n",
    "    seed: str; seed of the policy to test\n",
    "    test_env_name: str; name of the desired test env\n",
    "    ica: if testing SAR-RL, the ICA object\n",
    "    pca: if testing SAR-RL, the PCA object\n",
    "    normalizer: if testing SAR-RL, the normalizer object\n",
    "    phi: float; blend parameter between synergistic and nonsynergistic activations\n",
    "    episodes: int; number of episodes to run on the test environment\n",
    "    \"\"\"\n",
    "    if is_sar:\n",
    "        if syn_nosyn:\n",
    "            env = SynNoSynWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "        else:\n",
    "            # env = SynergyWrapper(gym.make(test_env_name), ica, pca, normalizer, phi)\n",
    "            env = SynergyWrapper(gym.make(test_env_name), ica, pca, normalizer)\n",
    "    else:\n",
    "        env = gym.make(test_env_name)\n",
    "    env.reset()\n",
    "\n",
    "    model = SAC.load(f'{name}_model_{env_name}_{seed}')\n",
    "    vec = VecNormalize.load(f'{name}_env_{env_name}_{seed}', DummyVecEnv([lambda: env]))\n",
    "    solved = []\n",
    "    for i,_ in enumerate(range(episodes)):\n",
    "        is_solved = []\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            o = env.get_obs()\n",
    "            o = vec.normalize_obs(o)\n",
    "            a, __ = model.predict(o, deterministic=determ)\n",
    "            next_o, r, done, *_, info = env.step(a)\n",
    "            is_solved.append(info['solved'])\n",
    "        \n",
    "        if sum(is_solved) > 0:\n",
    "            solved.append(1)\n",
    "        else:\n",
    "            solved.append(0)\n",
    "\n",
    "    env.close()\n",
    "    return np.mean(solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08578ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing RL-E2E policy\n",
    "\n",
    "name = 'RL-E2E'\n",
    "\n",
    "e2e_id = zeroshot_test(name, 'myoHandReorientID-v0')\n",
    "\n",
    "e2e_ood = zeroshot_test(name, 'myoHandReorientOOD-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c97370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zero-shot testing SAR-RL policy\n",
    "\n",
    "name = 'SAR-RL'\n",
    "\n",
    "sar_id = zeroshot_test(name, 'myoHandReorientID-v0', ica=ica, pca=pca, \n",
    "                       normalizer=normalizer, is_sar=True, syn_nosyn=True)\n",
    "\n",
    "sar_ood = zeroshot_test(name, 'myoHandReorientOOD-v0', ica=ica, pca=pca, \n",
    "                        normalizer=normalizer, is_sar=True, syn_nosyn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cfe1b",
   "metadata": {},
   "source": [
    "We can visualize these zero-shot generalization results by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18035c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "    \n",
    "zeroshot = {\"SAR-RL\": [sar_id,sar_ood],\n",
    "            \"RL-E2E\": [e2e_id,e2e_ood]}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_zeroshot(ax, zeroshot)\n",
    "fig.set_size_inches(6, 5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
